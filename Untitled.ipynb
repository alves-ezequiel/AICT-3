{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy.io import loadmat"
      ],
      "metadata": {
        "id": "SgNcli9sdJPT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "carregando os dados"
      ],
      "metadata": {
        "id": "9xYSEczQUric"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_radar_data(csv_file, radar_column='unit1_radar', label_column='unit1_beam_index'):\n",
        "    # Especifique o diretório do seu drive (por exemplo, \"/content/drive/MyDrive/Arquivos UFC\") abaixo\n",
        "    root_dir = '/content/drive/MyDrive/Arquivos UFC'\n",
        "\n",
        "    csv_file = pd.read_csv(os.path.join(root_dir, csv_file))\n",
        "\n",
        "    # Inicializa uma lista para armazenar os dados de radar\n",
        "    X = []\n",
        "\n",
        "    for i in tqdm(range(len(csv_file))):\n",
        "        if i >= 3000:\n",
        "            break  # Sai do loop após ler os primeiros 3000 arquivos\n",
        "        try:\n",
        "            # Tenta carregar o arquivo de radar\n",
        "            radar_data = loadmat(os.path.abspath(os.path.join(root_dir, csv_file[radar_column][i])))['data']\n",
        "            X.append(radar_data)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"O arquivo {csv_file[radar_column][i]} não foi encontrado e será ignorado.\")\n",
        "\n",
        "    # Converte a lista de arrays em um único array\n",
        "    X = np.array(X, dtype=X[0].dtype)\n",
        "\n",
        "    y = np.array(csv_file[label_column])\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "dZS15qmSigxX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = load_radar_data('/content/drive/MyDrive/Arquivos UFC/unit1/scenario9.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E81PgLZiilWy",
        "outputId": "24abcc61-e030-43ec-aa83-ddcb58d33638"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 3000/5964 [13:31<13:21,  3.70it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_verdade= y[:3000]\n"
      ],
      "metadata": {
        "id": "t7GiNuLxvLgS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "print(len(Counter(y_verdade)))"
      ],
      "metadata": {
        "id": "1O4t_HQgvg9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea0594f-4a1c-44cc-d1f8-8566d2330f25"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4vMvVUXSt2t",
        "outputId": "1e02c67f-3824-4771-b695-1a69cbc984d2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3000, 4, 256, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pre - processamento"
      ],
      "metadata": {
        "id": "c1uCXw8LUv7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def range_velocity_map(data):\n",
        "    size = list(data.shape)\n",
        "    size[1] = 1\n",
        "    new_data = np.zeros(tuple(size), dtype=np.float32)\n",
        "    for i in tqdm(range(data.shape[0])):\n",
        "        new_data[i, 0, :, :] = range_velocity_map_single(data[i])\n",
        "    return new_data\n",
        "\n",
        "\n",
        "def range_velocity_map_single(data):\n",
        "    data = np.fft.fft(data, axis=1) # Range FFT\n",
        "    data = np.fft.fft(data, axis=2) # Velocity FFT\n",
        "    data = np.abs(data).sum(axis=0) # Sum over antennas\n",
        "    return data"
      ],
      "metadata": {
        "id": "ctlBXtRkmMqx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "velocidade = range_velocity_map(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_oURppPaY7y",
        "outputId": "1f1b5a9d-f31f-443c-83f6-5a5ff5bc8baf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3000/3000 [00:31<00:00, 95.87it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(velocidade, y_verdade, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "I8zriBIFuhag"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (1, 1), padding='same', activation='relu', input_shape=(1, 256, 128)))\n",
        "model.add(Conv2D(32, (1, 1), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Dense(1, activation ='softmax'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(X_treinamento, y_treinamento, epochs=10, batch_size=8, validation_data=(X_teste, y_teste))"
      ],
      "metadata": {
        "id": "xVYhAIAGtnPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e94bdac-60e4-4737-d176-6a9580178a45"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_22 (Conv2D)          (None, 1, 256, 32)        4128      \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 1, 256, 32)        1056      \n",
            "                                                                 \n",
            " max_pooling2d_15 (MaxPooli  (None, 1, 256, 32)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " flatten_10 (Flatten)        (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 64)                524352    \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 532161 (2.03 MB)\n",
            "Trainable params: 532161 (2.03 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(8, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300/300 [==============================] - 5s 13ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "300/300 [==============================] - 3s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "300/300 [==============================] - 5s 15ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "300/300 [==============================] - 5s 16ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a2d7aba0880>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}